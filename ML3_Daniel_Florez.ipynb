{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. In your own words, describe what vector embeddings are and what they are useful for.\n",
        "\n",
        "\n",
        "Embendings are vectors that contain representative values for a file, which can be text, images, audio, etc. The objective is to be able to manage the main information in a lower dimensional vector to facilitate data management.\n",
        "The embedings manage to represent the data in different environments in a numerical and \"simple\" way. which facilitates the use of these methods to work in different industries."
      ],
      "metadata": {
        "id": "VCA-p5BC11Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each other? Why?\n",
        "\n",
        "There are different ways to calculate the distance between the embedings, for better understanding vectors in this case. The most popular and well-known form is the Euclidean, due to the ease of performing the calculations, this method is sensitive to scale and does not take into account the direction of the vector.\n",
        "\n",
        "For the topic of embeddings that we are reviewing, the cosine distance method is recommended, in Npl it is used because it takes into account the direction of the vector. which returns data between -1 and 1, according to the semantics of the analyzed text, in addition to being robust to the scale of the values.\n",
        "\n",
        "Depending on the environment in which the exercise takes place, different distances can be taken that behave better in scenarios than others. For example, the type of data, the problem to be solved."
      ],
      "metadata": {
        "id": "sBRod-0t2L0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Let us build a Q&A (question answering) system! 😀For this, consider the following steps:\n",
        "a. Pick whatever text you like, in the order of 20+ paragraphs.\n",
        "\n",
        "b. Split that text into meaningful chunks/pieces.\n",
        "\n",
        "c. Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?\n",
        "\n",
        "d. For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Selected Text\n",
        "https://www.vice.com/en/article/qjvwqx/death-athletic-documentary-jessica-solce-cody-wilson\n",
        "\n"
      ],
      "metadata": {
        "id": "bMI6vuuk3M4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "xcTtWRjq5BPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07VRr-stto3J",
        "outputId": "66760a60-0015-43c3-c9da-343f526a289d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BPtOtzdZM_S",
        "outputId": "946f51d4-c1c3-48af-9a5a-fe7e14c8086e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-22 21:59:30.762064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-22 21:59:32.512116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo A\n",
        "en_core_web_sm"
      ],
      "metadata": {
        "id": "xWuPBcMf5L4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from itertools import combinations\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "ruta = \"D-Printed Guns.txt\"\n",
        "\n",
        "with open(ruta, \"r\", encoding=\"iso-8859-1\") as archivo:\n",
        "    texto = archivo.read()\n",
        "\n",
        "def Que(Question):\n",
        "    Question_npl = nlp(Question)\n",
        "\n",
        "    parrafos = texto.strip().split(\"\\n\\n\")\n",
        "    similarity_scores = [Question_npl.similarity(nlp(parrafo)) for parrafo in parrafos]\n",
        "    parrafos_con_puntajes = list(zip(parrafos, similarity_scores))\n",
        "    parrafos_con_puntajes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (parrafo, puntaje) in enumerate(parrafos_con_puntajes[:3], 1):\n",
        "        print(f\"Párrafo {i}:\")\n",
        "        print(parrafo)\n",
        "        print(f\"Puntaje de similitud: {puntaje}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "CzD_9NWYfA2m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"What work do sowftare3d companies do?\"\n",
        "q = Que(Question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp66dilne-Zu",
        "outputId": "b197f324-944a-439a-8498-ba57eb273bfc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-838b8af7e016>:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_scores = [Question_npl.similarity(nlp(parrafo)) for parrafo in parrafos]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Párrafo 1:\n",
            "This bleeds into the First Amendment, which makes it super interesting. Can the government control information? How and to what level, especially when it's dispersed on the internetand we know when something's on the internet, it never comes off the internet? Part of my interest in following this case was to figure out how the government would support their idea that they could control code.\n",
            "Puntaje de similitud: 0.3936300838128631\n",
            "\n",
            "Párrafo 2:\n",
            "So Cody's story is one of this deep tradition of technopolitics. He created something, he put it online, and the ability to restrain that to silence or surveil is pretty impossible, no matter what's happened, and the film demonstrates that on a multitude of levels. \n",
            "Its been two years since you wrapped filming. What has happened since\n",
            "Puntaje de similitud: 0.3404164233167614\n",
            "\n",
            "Párrafo 3:\n",
            "What does the title Death Athletic mean?\n",
            "Death Athletic comes from the German philosopher Peter Sloterdijl. He's talking about death athletes as martyrs for religious purposes, but I kind of modernized it. To me, it is the idea of someone who looks death in the eye, and death is no longer tyrannical. It becomes emancipatory. Being death athletic, to me, is someone who is steadfast in their motivations and stands for their principles, despite the obvious retribution or anger they're going to inspire.\n",
            "Puntaje de similitud: 0.3194634341390741\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"How does the government intervene?\"\n",
        "q = Que(Question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx2Vyih-fq7n",
        "outputId": "4dd615d2-4655-40e5-fbdb-ba4fff7e7a06"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-838b8af7e016>:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_scores = [Question_npl.similarity(nlp(parrafo)) for parrafo in parrafos]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Párrafo 1:\n",
            "This bleeds into the First Amendment, which makes it super interesting. Can the government control information? How and to what level, especially when it's dispersed on the internetand we know when something's on the internet, it never comes off the internet? Part of my interest in following this case was to figure out how the government would support their idea that they could control code.\n",
            "Puntaje de similitud: 0.5233726250100923\n",
            "\n",
            "Párrafo 2:\n",
            "What is it about 3D weapons that frightens people so much when a person has hypothetically always had the right to make their own gun?\n",
            "I really think that's just the case of marketing. Of course, there is far more ease in 3D printers, and they will continue improving. When this all started 10 years ago, it produced a palpable fear because it was such a new tech.\n",
            "Puntaje de similitud: 0.4273708491500898\n",
            "\n",
            "Párrafo 3:\n",
            "To this day, the government has still only come after Cody. He made himself such an object of resistance that he became the only person they actually cared about. So I was also interested in how his legal case would resolve: whether he would go to jail and what else he would create. But I also really wanted to get into his motivations and ethics. I wanted to see more. I wanted to peek behind the curtain a little harder.\n",
            "Puntaje de similitud: 0.42421173392990175\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"What is the Mean idea of the article?\"\n",
        "q = Que(Question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrnTN7PJg9bX",
        "outputId": "0c2f27f9-9f61-427e-c495-ed29e3d411e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-838b8af7e016>:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_scores = [Question_npl.similarity(nlp(parrafo)) for parrafo in parrafos]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Párrafo 1:\n",
            "This bleeds into the First Amendment, which makes it super interesting. Can the government control information? How and to what level, especially when it's dispersed on the internetand we know when something's on the internet, it never comes off the internet? Part of my interest in following this case was to figure out how the government would support their idea that they could control code.\n",
            "Puntaje de similitud: 0.5646415498625348\n",
            "\n",
            "Párrafo 2:\n",
            "VICE: How did you become interested in telling this story?\n",
            "Jessica Solce: Somehow, I've wrapped myself into 10 years of being within this gun debate. My first film, No Control, was about the efficacy of gun control. Cody Wilson was in it, and he was a charismatic figure who was very much in the media at the time. When I finished the film, I thought: His story is going to continue to be interesting, and rather than be frustrated by all these ten-second clips that come out, let's really get a full version of what's happening. \n",
            "Puntaje de similitud: 0.5632147626969985\n",
            "\n",
            "Párrafo 3:\n",
            "Death Athletic, a new documentary, follows eight years of the ghost gun industry and its most controversial creator, Cody Wilson.\n",
            "Puntaje de similitud: 0.5542400129834595\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"What do you expect from the movie?\"\n",
        "q = Que(Question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKJccCcfhbbF",
        "outputId": "9b964123-a40b-44c0-95bd-326487c8332a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-d125349df473>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity_scores = [Question_npl.similarity(nlp(parrafo)) for parrafo in parrafos]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Párrafo 1:\n",
            "This bleeds into the First Amendment, which makes it super interesting. Can the government control information? How and to what level, especially when it's dispersed on the internetand we know when something's on the internet, it never comes off the internet? Part of my interest in following this case was to figure out how the government would support their idea that they could control code.\n",
            "Puntaje de similitud: 0.6550469721311116\n",
            "\n",
            "Párrafo 2:\n",
            "The documentary itself is very sparse and straightforward, with little text or explanation given to the viewer. How do you see your style as a filmmaker?\n",
            "I want to stay out of the film as much as possible, personally. I think the best compliment I've received so far is from somebody who watched it and said, I forgot I was watching a documentary. That's all I need to hear. In every documentary, a camera changes the room. It changes the mood. I ran on a very tight team. Sometimes it was just me. I want to create as authentically as possible, and when I make the next one, I'm going to strive even more to watch and be a voyeur. It's not journalism to me, in a sense. There is lots of research, and there are lots of things that I was doing throughout the entire process, but I want the movie to actually be something you can watch and absorb all the information through the actual character you're following.\n",
            "Puntaje de similitud: 0.5450965799317699\n",
            "\n",
            "Párrafo 3:\n",
            "To this day, the government has still only come after Cody. He made himself such an object of resistance that he became the only person they actually cared about. So I was also interested in how his legal case would resolve: whether he would go to jail and what else he would create. But I also really wanted to get into his motivations and ethics. I wanted to see more. I wanted to peek behind the curtain a little harder.\n",
            "Puntaje de similitud: 0.5180176122534464\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model B\n",
        "bert-large-uncased-whole-word-masking-finetuned-squad"
      ],
      "metadata": {
        "id": "pN0A1gjc5cZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlpa = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\", tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "ruta = \"D-Printed Guns.txt\"\n",
        "\n",
        "with open(ruta, \"r\", encoding=\"iso-8859-1\") as archivo:\n",
        "    texto = archivo.read()\n",
        "\n",
        "pregunta = \"What new technologies do they talk about in the text?\"\n",
        "\n",
        "respuesta = nlpa(question=pregunta, context=texto)\n",
        "\n",
        "# Imprimir la respuesta\n",
        "print(f\"Question: {pregunta}\")\n",
        "print(f\"Ask: {respuesta['answer']}\")\n",
        "print(f\"Confidence score: {respuesta['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO7c7i36rSe5",
        "outputId": "1a83fd40-fa0c-47b7-eb73-b48ea70c50b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What new technologies do they talk about in the text?\n",
            "Ask: Defense Distributed, 3D guns, ghost gunsbut\n",
            "Confidence score: 0.0494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"When did the documentary start to be recorded?\"\n",
        "\n",
        "respuesta = nlpa(question=pregunta, context=texto)\n",
        "\n",
        "print(f\"Question: {pregunta}\")\n",
        "print(f\"Ask: {respuesta['answer']}\")\n",
        "print(f\"Confidence score: {respuesta['score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xReOE8LP-7PV",
        "outputId": "2dbb56d3-b6c5-472a-c8b6-38fe52fb9f92"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When did the documentary start to be recorded?\n",
            "Ask: March 2015\n",
            "Confidence score: 0.9137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which tools and approaches would help you generate them asily and high-level?\n",
        "\n",
        "As first we can use pre-trained models to help us save work, there are already models that have been evaluated and are available. And for better behavior in specific contects, it is possible to generate embendings based on a set of words together or to generate personalized embendings.\n",
        "It is also important to note that to handle these models, having good machines helps a lot."
      ],
      "metadata": {
        "id": "IKNlAu1m8LwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do results make sense?\n",
        "\n",
        "Yes, for the first model the paragraphs are related to the selected question and in the second model I find that the answers are well answered."
      ],
      "metadata": {
        "id": "QAvbXiZ_-WKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. What do you think that could make these types of systems more robust in terms of semantics and functionality?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The ways to robust these systems that I find are aimed at training the model to behave better in more specific environments. For example, using embeddings that better fit the contexts, using pre-trained models to perform certain tasks, feeding the system with valuable information to respond in a better way.\n",
        "Also in general terms there is feedback and evaluation of the systems that we are using or already looking for great things, you can think about integrating a multimodal system"
      ],
      "metadata": {
        "id": "dGgD2uMA7tXY"
      }
    }
  ]
}